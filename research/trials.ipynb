{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\NLP\\\\End-to-End-Source-Code-Analysis-Generative-AI-Project\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/mehboobme/Medical_Chatbot.git\", to_path = repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                       glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language=Language.PYTHON, parser_threshold=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, request, jsonify, render_template\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import prompt\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY= os.environ.get(\"PINECONE_API_KEY\")\\nOPENAI_API_KEY=os.environ.get(\"OPENAI_API_KEY\")\\n\\nembeddings = download_hugging_face_embeddings()\\n\\nindex_name = \"medicalbot\"\\n\\n#Embed each chunk and upsert the embeddings into your Pinecone index\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding = embeddings\\n)\\n\\n# define retriever to retrieve relevant information from Pinecone vector DB\\nretriever = docsearch.as_retriever(search_type=\\'similarity\\', search_kwargs={\"k\":3})\\n\\n#augement your result with RAG, OPEN AI LLM will give you refined answer\\nfrom langchain_openai import OpenAI\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n@app.route(\\'/\\')\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n@app.route(\\'/get\\', methods=[\\'GET\\', \\'POST\\'])\\ndef chat():\\n    msg = request.form[\\'msg\\']\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response: \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name= \"Generative AI Project\",\\n    version= \\'0.0.0\\',\\n    author= \\'Mahboob Khan\\',\\n    author_email = \\'mehboobmeo@gmail.com\\',\\n    packages = find_packages,\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY= os.environ.get(\"PINECONE_API_KEY\")\\n\\nextracted_data=load_pdf_file(data=\\'.\\')\\ntext_chunks= text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\nindex_name = \"medicalbot\"\\n\\npc.create_index(\\n    name = index_name,\\n    dimension=384,\\n    metric=\\'cosine\\',\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\\'us-east-1\\'\\n    )\\n)\\n\\n# Embed each chunk and upsert the embeddings into your Pineconre index\\nfrom langchain_pinecone import PineconeVectorStore\\n\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n# Set up logging\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",    \\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath)==0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n# Extract Data from the PDF file\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(data,\\n                             glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    documents = loader.load()\\n\\n    return documents\\n\\n#split the data into text chunks\\ndef text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n# Download the Embeddings from Hugging Face\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name= \\'sentence-transformers/all-MiniLM-L6-v2\\')\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain_core.prompts import ChatPromptTemplate\\n\\n# Define the system prompt\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you do not know the answer, say that you \"\\n    \"do not know. Use three sentences maximum and keep the answer concise.\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n\\n# Create the prompt template\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                                                  chunk_size = 500,\n",
    "                                                                  chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, request, jsonify, render_template\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import prompt\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\n\\nPINECONE_API_KEY= os.environ.get(\"PINECONE_API_KEY\")\\nOPENAI_API_KEY=os.environ.get(\"OPENAI_API_KEY\")\\n\\nembeddings = download_hugging_face_embeddings()\\n\\nindex_name = \"medicalbot\"\\n\\n#Embed each chunk and upsert the embeddings into your Pinecone index\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding = embeddings\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='# define retriever to retrieve relevant information from Pinecone vector DB\\nretriever = docsearch.as_retriever(search_type=\\'similarity\\', search_kwargs={\"k\":3})\\n\\n#augement your result with RAG, OPEN AI LLM will give you refined answer\\nfrom langchain_openai import OpenAI\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n@app.route(\\'/\\')'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n@app.route(\\'/get\\', methods=[\\'GET\\', \\'POST\\'])\\ndef chat():\\n    msg = request.form[\\'msg\\']\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response: \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name= \"Generative AI Project\",\\n    version= \\'0.0.0\\',\\n    author= \\'Mahboob Khan\\',\\n    author_email = \\'mehboobmeo@gmail.com\\',\\n    packages = find_packages,\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY= os.environ.get(\"PINECONE_API_KEY\")\\n\\nextracted_data=load_pdf_file(data=\\'.\\')\\ntext_chunks= text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='pc = Pinecone(api_key=PINECONE_API_KEY)\\nindex_name = \"medicalbot\"\\n\\npc.create_index(\\n    name = index_name,\\n    dimension=384,\\n    metric=\\'cosine\\',\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\\'us-east-1\\'\\n    )\\n)\\n\\n# Embed each chunk and upsert the embeddings into your Pineconre index\\nfrom langchain_pinecone import PineconeVectorStore\\n\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n# Set up logging\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",    \\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath)==0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n# Extract Data from the PDF file\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(data,\\n                             glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    documents = loader.load()\\n\\n    return documents\\n\\n#split the data into text chunks'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n# Download the Embeddings from Hugging Face\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name= 'sentence-transformers/all-MiniLM-L6-v2')\\n    return embeddings\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain_core.prompts import ChatPromptTemplate\\n\\n# Define the system prompt\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you do not know the answer, say that you \"\\n    \"do not know. Use three sentences maximum and keep the answer concise.\\\\n\\\\n\"\\n    \"{context}\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Create the prompt template\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = documents_splitter.split_documents(documents)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings= OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorDB= Chroma.from_documents(texts, embedding=embeddings, persist_directory='db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorDB.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use llm\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectorDB.as_retriever(search_type='mmr', search_kwargs={\"k\":8}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is langchain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is a framework that combines different AI models and tools to perform complex natural language processing tasks. It includes components like retrievers for accessing relevant information, language models for generating answers, and chains that link these components together to provide accurate and efficient question-answering capabilities.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
